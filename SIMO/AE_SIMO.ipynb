{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AE_SIMO.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python [conda env:DLJ]","language":"python","name":"conda-env-DLJ-py"}},"cells":[{"metadata":{"collapsed":true,"id":"G_OetUucUSWP","colab_type":"text"},"cell_type":"markdown","source":["# SIMO model - Training an end-to-end communications system on a Rayleigh-Fading channel based on an autoencoder model\n","\n","This model is modified explicitly for the SIMO scenario\n","       \n","Hint: To prevent memory garbage on disk suppress summary generation when not explicitly desired\n","\n","[ Comment: The source code of Dr.-Ing. Jakob Hoydis' realisation of the autoencoder model from the paper \n","\"An Introduction to Deep Learning for the Physical Layer\" (Timothy O’Shea; Jakob Hoydis, 02 October 2017) was used as a basis ] "]},{"metadata":{"collapsed":true,"id":"mrqIxKqgUSWR","colab_type":"text"},"cell_type":"markdown","source":["## Import Libraries\n"]},{"metadata":{"id":"aCFWvb-rUSWV","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import time\n","#%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import warnings\n","with warnings.catch_warnings():\n","    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","    import tensorflow as tf\n","import os\n","import pickle\n","import itertools as it\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Ignore if you do not have multiple GPUs\n"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"9woOC41JUSWe","colab_type":"text"},"cell_type":"markdown","source":["## This section defines the parameters which need to be fed to the model for the initializiation process\n","\n","- Be aware that the parameters influence the size of the neural networks\n","- k is the number of information bits per message, note that M = 2**k\n","- n is the number of complex channel uses per message, is here fixed to n=1 !!\n","- seed can be used to reproduce identical results by fixing the seed of the random number generators, note that the line \"tf.set_random_seed(self.seed)\" should be commented out to set the seed in the graph, otherwise 'seed' is only used to distinguish between trained models in the saving process\n","- mt and mr are the number of transmit and recive antenna respectively, in SIMO mt is fixed to 1\n","- model_file_1 and model_file_2 are only declared to distinguish models in saving processes\n"]},{"metadata":{"id":"xmx1TRsOUSWg","colab_type":"code","colab":{}},"cell_type":"code","source":["k = 2  #Number of information bits per message, i.e., M=2**k\n","n = 1 # Number of complex channel uses per message\n","seed = 99 # Seed RNG reproduce identical results\n","mt = 1\n","mr = 3\n","\n","assert n is 1, \"n does not equal 1\"\n","assert mt is 1, \"mt does not equal 1\"\n","\n","model_file_1 = 'SIMO/' \n","model_file_2 = '_k_{}_n_{}_tx_{}_rx_{}_s_{}'.format(k, n, mt, mr, seed)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GWbce8HKUSWn","colab_type":"text"},"cell_type":"markdown","source":["## Build Autoencoder with subfunctions "]},{"metadata":{"collapsed":true,"id":"-QM_bCFrUSWp","colab_type":"text"},"cell_type":"markdown","source":["\n"]},{"metadata":{"id":"bWozBC91USWr","colab_type":"code","colab":{}},"cell_type":"code","source":["class AE(object):\n","    def __init__(self, k, n, mt, mr, seed=None, filename=None):\n","        '''This function is the initialization function of the Autoencoder class'''\n","        self.k = k\n","        self.n = n\n","        self.mt = mt\n","        self.mr = mr\n","        self.bits_per_symbol = self.k / self.n\n","        self.M = 2 ** self.k\n","        self.T = 2 ** (self.mt * self.k)  # Anzahl möglicher Tupel\n","        self.seed = seed if (seed is not None) else int(time.time())\n","        self.graph = None\n","        self.sess = None\n","        self.vars = None\n","        self.saver = None\n","        self.constellations = None\n","        self.blers = None\n","        self.create_graph()\n","        self.create_session()\n","        self.sum = self.sum_writer()\n","        if filename is not None:\n","            self.load(filename)\n","        return\n","\n","    def create_graph(self):\n","        '''This function creates the computation graph of the autoencoder'''\n","        self.graph = tf.Graph()\n","        with self.graph.as_default():\n","            # Comment out if you want to set the RNG seed\n","            #tf.set_random_seed(self.seed)\n","            \n","            # Placeholder which defines the input dimension of the AE\n","            batch_size = tf.placeholder(tf.int32, shape=())\n","\n","            # Create Channelmatrix\n","            hc, h_resh = self.channel_matrix()\n","\n","            # Transmitter Functions\n","            s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n","            x, x_plot, vars_enc = self.encoder(s, h_resh)\n","\n","            # Channel\n","            noise_std = tf.placeholder(tf.float32, shape=())\n","            y = self.channel(x, hc, noise_std)\n","\n","            # Receiver\n","            yc_rtn, h_hat, _ = self.predecode(y, h_resh)\n","            s_hat, vars_dec = self.decoder(yc_rtn)\n","\n","            # Loss function\n","            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n","\n","            # Performance metrics\n","            preds = tf.nn.softmax(s_hat)\n","            s_hat_bit = tf.argmax(preds, axis=1)\n","            correct_predictions = tf.equal(s_hat_bit, s)\n","            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n","            bler = 1 - accuracy\n","            \n","            with tf.name_scope('performance'):\n","                tf.summary.scalar('loss', cross_entropy)\n","                tf.summary.scalar('bler', bler)\n","\n","            # Optimizer\n","            lr = tf.placeholder(tf.float32, shape=())  # We can feed in any desired learning rate for each step\n","            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n","            \n","            # Handle Training Parameters to monitor them\n","            train_vars = tf.trainable_variables()\n","            train_v = [vars_enc, vars_dec]\n","            \n","            merged = tf.summary.merge_all()\n","\n","            # References to graph variables we need to access later\n","            self.vars = {\n","                'accuracy': accuracy,\n","                'batch_size': batch_size,\n","                'bler': bler,\n","                'cross_entropy': cross_entropy,\n","                'init': tf.global_variables_initializer(),\n","                'lr': lr,\n","                'noise_std': noise_std,\n","                'train_op': train_op,\n","                's': s,\n","                's_hat': s_hat,\n","                'x': x,\n","                'h': hc,\n","                'y': y,\n","                'x_plot': x_plot,\n","                'hc_plot': hc,\n","                's_hat_bit': s_hat_bit,\n","                'preds': preds,\n","                'merged': merged,\n","                'train_vars': train_vars,\n","                'train_v': train_v,\n","                'yc_rtn': yc_rtn,\n","                'h_hat': h_hat,\n","                'corr_preds': correct_predictions\n","            }\n","            self.saver = tf.train.Saver()\n","        return\n","\n","    def create_session(self):\n","        '''Create a session for the autoencoder instance with the compuational graph'''\n","        self.sess = tf.Session(graph=self.graph)\n","        self.sess.run(self.vars['init'])\n","        return\n","\n","    def channel_matrix(self):\n","        '''The Channel Matrix Calculation (MIMO-Rayleigh Fading Channel)'''\n","        \n","        # Define standard deviation as Rayleigh\n","        h_std_r = np.sqrt(0.5)\n","        h_mean_r = 0\n","        h_std_i = np.sqrt(0.5)\n","        h_mean_i = 0\n","        \n","        # Calculate channel symbols\n","        hr = tf.random_normal([self.mt, self.mr], mean=h_mean_r, stddev=h_std_r, dtype=tf.float32)\n","        hi = tf.random_normal([self.mt, self.mr], mean=h_mean_i, stddev=h_std_i, dtype=tf.float32)\n","        hc = tf.complex(hr, hi)\n","        \n","        # Channel in real notation\n","        h = tf.concat([tf.real(hc), tf.imag(hc)], axis=1)\n","        h_resh = tf.reshape(h, shape=[1, -1])\n","\n","        return hc, h_resh\n","\n","    def encoder(self, input, h):\n","        '''The transmitter (Encoder-NN)'''\n","        \n","        # Embedding Layer  \n","        with tf.name_scope('enc_1'):\n","            # Embedding Matrix\n","            W = self.weight_variable((self.M, self.M))\n","            # Embedding Look-Up and ELU-Activation\n","            x = tf.nn.elu(tf.nn.embedding_lookup(W, input, name='enc_1'))\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_enc_1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'enc_1')\n","        \n","        # Hidden Layer\n","        with tf.name_scope('enc_2'):\n","            # Dense Layer\n","            x = tf.layers.dense(x, self.mt * self.M, activation=tf.nn.leaky_relu, name='enc_2')\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_enc_2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'enc_2')\n","               \n","        # Hidden Layer\n","        with tf.name_scope('enc_3'):\n","            # Dense Layer\n","            x = tf.layers.dense(x, 2 * self.mt * self.mt, activation=tf.nn.leaky_relu, name='enc_3')\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_enc_3 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'enc_3')\n","            \n","        # Output Layer\n","        with tf.name_scope('enc_4'):\n","            # Dense Layer\n","            x = tf.layers.dense(x, 2 * self.mt * self.mt, activation=None, name='enc_4')\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_enc_4 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'enc_4')\n","        \n","        # Concat all summaries of the layer parameters     \n","        vars = [var_enc_1, var_enc_2, var_enc_3, var_enc_4]\n","        \n","        # Real to complex transformation\n","        x = tf.reshape(x, shape=[-1, 2])\n","        x = tf.reshape(tf.complex(x[::, 0], x[::, 1]), shape=[-1, 1])\n","         \n","        # Average power normalization over Set\n","        norma = tf.sqrt(tf.reduce_mean(tf.square(tf.abs(x)), axis=0))\n","        x = x / tf.cast(norma, dtype=tf.complex64)    \n","       \n","        # x_plot might need to be reshaped for other plotting purposes  \n","        x_plot = x\n","                \n","        return x, x_plot, vars\n","\n","    def channel(self, input, hc, noise_std):\n","        '''The channel (Rayleigh-Fading Channel)'''\n","        \n","        # Channel Matrix\n","        yc = tf.matmul(input, hc)\n","        \n","        # Noise\n","        nr = tf.random_normal(tf.shape(yc), mean=0.0, stddev=(noise_std/np.sqrt(2)), dtype=tf.float32)\n","        ni = tf.random_normal(tf.shape(yc), mean=0.0, stddev=(noise_std/np.sqrt(2)), dtype=tf.float32)\n","        nc = tf.complex(nr, ni)\n","        yc = tf.add(yc, nc)\n","        \n","        return yc\n","    \n","    def predecode(self, yc, h):\n","        '''Predecoder-NN with Zero-Forcing Orientation (Input: Received Symbols plus CSI)'''\n","        \n","        # Complex to real Transformation\n","        y = tf.reshape(yc, shape=[-1, 1])\n","        y = tf.concat([tf.real(y), tf.imag(y)], axis=1)     \n","        y = tf.reshape(y, shape=[-1, 2])\n","\n","        # Concat CSIR to y\n","        h = tf.tile(h, [tf.shape(y)[0], 1])\n","        input = tf.concat([y, h], 1)\n","           \n","        # Input Layer\n","        with tf.name_scope('pre_1'):\n","            # Dense Layer\n","            h_hat = tf.layers.dense(input, 12, activation=tf.tanh, name='pre_1')\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_pre_1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'pre_1')\n","        \n","        # Output Layer\n","        with tf.name_scope('pre_2'):\n","            # Dense Layer\n","            h_hat = tf.layers.dense(h_hat, 2 * self.mr, activation=None, name='pre_2')\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_pre_2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'pre_2')\n","        \n","        # Concat all summaries of the layer parameters \n","        vars = [var_pre_1, var_pre_2]\n","        \n","        # Real to Complex and reshape to right Dimensions\n","        h_hat = tf.reshape(h_hat, shape=[-1, 2])\n","        h_hat = tf.complex(h_hat[:, 0], h_hat[:, 1])\n","        h_hat = tf.reshape(h_hat, shape=[-1, self.mr])\n","        h_hat = tf.reduce_mean(h_hat, axis=0)\n","        h_hat = tf.reshape(h_hat, shape=[self.mr, -1])\n","        \n","        # Multiplication of learned Matrix with received Symbols\n","        yc_rtn = tf.matmul(yc, h_hat)\n","        \n","        return yc_rtn, h_hat, vars\n","          \n","    def decoder(self, yc):\n","        '''The Receiver (Decoder-NN)'''\n","        \n","        # Complex to real plus reshape to Dimension as in Output Vector of Encoder-NN\n","        yc = tf.reshape(yc, shape=[-1, 1])\n","        y = tf.concat([tf.real(yc), tf.imag(yc)], axis=1)\n","        y = tf.reshape(y, shape=[-1, 2 * self.mt])\n","\n","        input = y\n","        \n","        # Input Layer\n","        with tf.name_scope('dec_1'):\n","            # Dense Layer\n","            y = tf.layers.dense(input, self.mr*self.M, activation=tf.nn.leaky_relu, name='dec_1')\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_dec_1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'dec_1')\n","        \n","        # Hidden Layer\n","        with tf.name_scope('dec_2'):\n","            # Dense Layer\n","            y = tf.layers.dense(y, self.mr*self.M, activation=tf.nn.leaky_relu, name='dec_2')\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_dec_2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'dec_2')\n","            \n","        # Hidden Layer\n","        with tf.name_scope('dec_3'):\n","            # Dense Layer\n","            y = tf.layers.dense(y, self.mr*self.M, activation=tf.nn.leaky_relu, name='dec_3')\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_dec_3 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'dec_3')\n","            \n","        # Output Layer\n","        with tf.name_scope('dec_4'):\n","            # Dense Layer\n","            y = tf.layers.dense(y, self.M, activation=None, name='dec_4')\n","            # Store layer parameters to monitor them in Tensorboard\n","            var_dec_4 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'dec_4')\n","        \n","        # Concat all summaries of the layer parameters \n","        vars = [var_dec_1, var_dec_2, var_dec_3, var_dec_4]\n","       \n","        return y, vars\n","\n","    def EbNo2Sigma(self, ebnodb):\n","        '''Convert Eb/No in dB to noise standard deviation'''\n","        ebno = 10**(ebnodb/10)\n","        return np.sqrt(1 / (self.bits_per_symbol * ebno))\n","\n","    def gen_feed_dict(self, batch_size, ebnodb, lr):\n","        '''Generate a feed dictionary for training and validation'''\n","        return {\n","            self.vars['batch_size']: batch_size,\n","            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n","            self.vars['lr']: lr,\n","        }\n","\n","    def test_step(self, batch_size, ebnodb):\n","        '''Compute the BLER over a single batch and Eb/No'''\n","        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n","        return bler\n","\n","    def plot_constellation(self, maxrange=None):\n","        '''Generate a plot of the current constellation'''\n","              # Get symbols \n","        x = np.reshape(self.transmit(range(self.M)), (self.M, self.n))\n","        \n","        # Define plot parameters\n","        if (maxrange is None):\n","            maxrange = np.max(np.abs(x))\n","        print('Constellation points: ' + str(x))\n","        colours = cm.cmap_d['gist_rainbow'](np.linspace(0, 1, self.M)) \n","        image = plt.figure(figsize=(6, 6))\n","        plt.grid(True)\n","        plt.xlim(-maxrange,maxrange)\n","        plt.ylim(-maxrange,maxrange)\n","        \n","        # Calculate bit-block assignement\n","        bit_assigned = self.assign_bits_to_msg(100, 30, 10000)\n","        \n","        # Plot symbols\n","        for j in range(self.M):\n","            for k in range(self.n):\n","                plt.scatter(x[j, k].real, x[j, k].imag, c=colours[j], label=bit_assigned[j] ,marker='x', s=75)\n","        \n","        plt.legend(bit_assigned, loc='upper center', bbox_to_anchor=(1.2,1), ncol=1, fontsize=16, handletextpad=0.05, labelspacing=0.2)\n","        parameter = {\"ytick.color\": \"k\", \"xtick.color\": \"k\", \"axes.labelcolor\": \"k\", \"axes.edgecolor\": \"k\"}\n","        plt.xlabel('I', fontsize=18)\n","        plt.ylabel('Q', fontsize=18)\n","        plt.tick_params(labelsize=16)\n","        plt.locator_params(nbins=5)\n","        plt.rcParams.update(parameter)\n","        image.axes[0].set_xticks(np.array([-2, -1, 0, 1, 2]))                                          \n","        image.axes[0].set_yticks(np.array([-2, -1, 0, 1, 2]))\n","        return x, image\n","    \n","    def plot_receive(self, ebn0, channel_realizations, rtn=0, maxrange=None):\n","        '''Generate a plot of the received constellation (Be Aware: with rtn = 0 or 1 you influence \n","                    whether you want to plot the symbols before or after the Predecoder-NN'''\n","        \n","        # Get Symbols\n","        if rtn == 1:\n","            y = self.receive_RTN(ebn0, channel_realizations)\n","        else:\n","            y = self.receive(ebn0, channel_realizations)\n","        if (maxrange is None):\n","            maxrange = np.max(np.abs(y))\n","        \n","        # Calculate bit assignment and init color map   \n","        colours = cm.cmap_d['gist_rainbow'](np.linspace(0, 1, self.T)) \n","        bit_assigned = self.assign_bits_to_msg(100, 30, 10000)\n","        \n","        # Generate Plot for received Symbols\n","        if rtn == 0:\n","            image = plt.figure(figsize=(12, (12/self.mr)))\n","            for i in range(self.mr):\n","                ax = image.add_subplot(1, self.mr, i+1)\n","                \n","                ax.grid(True)\n","                ax.set_xlim(-maxrange, maxrange)\n","                ax.set_ylim(-maxrange, maxrange)\n","                for j in range(channel_realizations):\n","                    symbols = y[j]\n","                    for k in range(self.T):\n","                        if j == 0 and i == 0:\n","                            ax.scatter(symbols[k, i].real, symbols[k, i].imag, c=colours[k], marker='x', label='s{}'.format(k), s=75)\n","                        else:\n","                            ax.scatter(symbols[k, i].real, symbols[k, i].imag, c=colours[k], marker='x')\n","                            \n","                title = 'Antenne {} '.format(i+1)\n","                ax.set_title(title, fontsize=18)\n","                ax.set_xlabel('I', fontsize=18)\n","                ax.set_ylabel('Q', fontsize=18)\n","                ax.tick_params(labelsize=16)\n","                ax.set(adjustable='box-forced', aspect='equal')\n","        \n","        # Generate Symbols in signal diagramm after the Predecoder-NN        \n","        if rtn == 1:\n","            image = plt.figure(figsize=(6, 6))\n","            for j in range(channel_realizations):\n","                    symbols = y[j]\n","                    for k in range(self.T):\n","                        if j == 0:\n","                            plt.scatter(symbols[k].real, symbols[k].imag, c=colours[k], marker='x', label='s{}'.format(k), s=75)\n","                        else:\n","                            plt.scatter(symbols[k].real, symbols[k].imag, c=colours[k], marker='x')\n","            plt.grid(True)\n","            plt.xlim(-maxrange, maxrange)\n","            plt.ylim(-maxrange, maxrange)\n","        \n","        plt.legend(bit_assigned, loc='upper center', bbox_to_anchor=(1.2,1), ncol=1, fontsize=16, handletextpad=0.05, labelspacing=0.2)\n","        parameter = {\"ytick.color\": \"k\", \"xtick.color\": \"k\", \"axes.labelcolor\": \"k\", \"axes.edgecolor\": \"k\"}\n","        plt.xlabel('I', fontsize=18)\n","        plt.ylabel('Q', fontsize=18)\n","        plt.tick_params(labelsize=16)\n","        image.tight_layout()\n","        plt.rcParams.update(parameter)\n","        \n","        return y\n","    \n","    def transmit(self, s):\n","        '''Returns the transmitted sigals corresponding to message indices'''\n","        return self.sess.run(self.vars['x_plot'], feed_dict={self.vars['s']: s})\n","    \n","    def receive(self, ebnodb, channel_realizations):\n","        '''Returns the received signals of multiple sending of the M consecutive messages (BE AWARE: Uses y_plot and not y!)\n","                            Returns: Array of M x channel_realizations'''\n","        y_ch = []\n","        s = range(self.M)\n","        noise_std = self.EbNo2Sigma(ebnodb)\n","        for i in range(channel_realizations):\n","            y = self.sess.run(self.vars['y'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n","            y_ch.append(y)        \n","        \n","        return y_ch\n","    \n","    def receive_RTN(self, ebnodb, channel_realizations):\n","        '''Returns the symbols after the RTN by multiple sending of the M consecutive messages \n","                            Returns: Array of M x channel_realizations'''\n","        y_ch = []\n","        s = range(self.M)\n","        noise_std = self.EbNo2Sigma(ebnodb)\n","        for i in range(channel_realizations):\n","            y = self.sess.run(self.vars['yc_rtn'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n","            y_ch.append(y)        \n","        \n","        return y_ch\n","\n","    def train(self, training_params, validation_params):\n","        '''Main training function, which handles train and validation paramters'''\n","        \n","        # Training and validation loop\n","        for index, params in enumerate(training_params):\n","            batch_size, lr, ebnodb, iterations, repeats = params\n","            print('\\nBatch Size: ' + str(batch_size) +\n","                  ', Learning Rate: ' + str(lr) +\n","                  ', EbNodB: ' + str(ebnodb) +\n","                  ', Iterations: ' + str(iterations) +\n","                  ', Repeats: ' + str(repeats))\n","\n","            val_size, val_ebnodb, val_steps = validation_params[index]\n","            \n","            for j in range(repeats):          \n","                for i in range(iterations):\n","                    \n","                    # Call training function\n","                    self.train_step(batch_size, ebnodb, lr)\n","                    \n","                    # Observe  val results\n","                    if i%val_steps==0 and index==0 and j==0:\n","                        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n","                        print(bler)\n","           \n","            # One last BLER Evaluation\n","            bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n","            print(bler)\n","        return\n","\n","    def train_step(self, batch_size, ebnodb, lr):\n","        '''A single training step: Computes one iteration of the graph and  optimizes neural net paramters'''\n","        summary, _ = self.sess.run([self.vars['merged'], self.vars['train_op']], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n","        return summary\n","    \n","    def sum_writer(self):\n","        '''Summary writer function which is needed for Tensorboard visualization'''\n","        sum_writer = tf.summary.FileWriter(model_file_1 + 'Summary', self.sess.graph)\n","        return sum_writer\n","\n","    def get_graphkeys(self):\n","        '''Get the trained weights and biases of the layers'''\n","        theta = self.sess.run(self.vars['train_v'])\n","        return theta\n","\n","    def weight_variable(self, shape):\n","        '''Xavier-initialized weights in Embedding Matrix'''\n","        (fan_in, fan_out) = shape\n","        low = np.sqrt(6.0 / (fan_in + fan_out))\n","        high = -np.sqrt(6.0 / (fan_in + fan_out))\n","        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n","    \n","    def save(self, filename):\n","        '''Save the current model'''\n","        return self.saver.save(self.sess, filename)\n","    \n","    def load(self, filename):\n","        '''Load an pre_trained model'''\n","        return self.saver.restore(self.sess, filename)\n","    \n","    def bler_sim(self, ebnodbs, batch_size, iterations):\n","        '''Monte Carlo simulations of BLER for a range of Eb/No, averaged over iterations with one iteration having size batch_size'''\n","        BLER = np.zeros_like(ebnodbs)\n","        for i in range(iterations):\n","            bler = np.array([self.sess.run(self.vars['bler'],\n","                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n","            BLER = BLER + bler/iterations\n","        return BLER\n","    \n","    def plot_bler(self, EbNodB, BLER, print):\n","        '''Plot a BLER curve'''\n","        image = plt.figure(figsize=(10,8))\n","        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n","        plt.yscale('log')\n","        plt.xlabel('EbNo (dB)', fontsize=18)\n","        plt.ylabel('Block-error rate', fontsize=18)\n","        plt.grid(True)\n","        plt.ylim([1e-5, 1])\n","        parameter = {\"ytick.color\": \"k\", \"xtick.color\": \"k\", \"axes.labelcolor\": \"k\", \"axes.edgecolor\": \"k\"}\n","        plt.rcParams.update(parameter)\n","        plt.legend(['Autoencoder'], prop={'size': 16}, loc='upper right')\n","        return image\n","\n","    def assign_bits_to_msg(self, batch_size, ebnodb, channel_realisations):\n","        '''Compute bit assignment to message indices: Get average BER over batchsize on multiple channelrealizations \n","                for specific EbN0; Take the bit assignment which produces lowest BER'''\n","        \n","        prob_arr = []\n","        # Average the prediction probabilities for each message when decoded over a loop\n","        for i in range(channel_realisations):            \n","            prob = np.zeros((self.M, self.M))\n","            s, preds = self.sess.run([self.vars['s'], self.vars['preds']], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n","            \n","            for ind, msg in enumerate(s):\n","                prob[msg] = prob[msg] + preds[ind]\n","                \n","            unique, counts = np.unique(s, return_counts=True)\n","            occurence = dict(zip(unique, counts))           \n","        \n","            for msg, cnts in occurence.items():\n","                if (cnts == 0):\n","                    prob[msg] = prob[msg]\n","                else:\n","                    prob[msg] = prob[msg] / cnts\n","                    \n","            prob_arr.append(prob)\n","            \n","        prob = np.mean(prob_arr, axis=0)\n","        \n","        # Try out all bit assignments possible for the bitblocks and compute BER\n","        bit_one = []\n","        bit_list = []\n","        msg_list = list(map(list, it.product([0, 1], repeat=self.k)))\n","        \n","        for k in range(len(msg_list)):\n","            msg_list = np.roll(msg_list, k)\n","            bit_list.append(msg_list)\n","            bit_err = []\n","            for i in range(len(msg_list)):\n","                for j in range(len(msg_list)):\n","                    # Compute Hamming Distance of the bitblock assignment (== # of bit errors if wrongly predicted)\n","                    hamming_dist = np.sum(np.bitwise_xor(msg_list[i], msg_list[j]))\n","                    bit_err.append(hamming_dist)\n","            \n","            # Calculate BER with hamming dist matrix and prediction probabilities              \n","            bit_err = np.reshape(np.asarray(bit_err), (self.M, self.M))\n","            bit_prob = np.multiply(prob, bit_err) / (self.M*self.k)\n","            bit_one.append(np.mean(bit_prob))\n","        \n","        # Check for the assigmnent with lowest BER\n","        index_min_BER = np.argmin(bit_one)\n","        assigned_bits = bit_list[int(index_min_BER)]\n","               \n","        return assigned_bits\n","    \n","    def calc_BER(self, batch_size, ebnodb, channel_realisations, assigned_bits):\n","        '''Calculate average BER over a ebno-level with batchsize messages per iterations \n","                            and over # of channel realizations (Monte-Carlo Simulation)'''\n","        \n","               \n","        bit_err = []\n","        prob_arr = []\n","        errorsum = 0\n","        \n","        # Average the prediction probabilities for each message when decoded over a loop\n","        for i in range(channel_realisations):            \n","            prob = np.zeros((self.M, self.M))\n","            \n","            # Run graph and get predictions\n","            s, preds, corr_preds = self.sess.run([self.vars['s'], self.vars['preds'], self.vars['corr_preds']], \n","                                     feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n","            \n","            for ind, msg in enumerate(s):\n","                prob[msg] = prob[msg] + preds[ind]\n","                \n","            unique, counts = np.unique(s, return_counts=True)\n","            occurence = dict(zip(unique, counts))           \n","        \n","            for msg, cnts in occurence.items():\n","                if (cnts == 0):\n","                    prob[msg] = prob[msg]\n","                else:\n","                    prob[msg] = prob[msg] / cnts\n","                    \n","            prob_arr.append(prob)\n","            \n","            # Count absolute message errors\n","            block_errors_batch = batch_size - np.sum(corr_preds)\n","            errorsum += block_errors_batch\n","            \n","        prob = np.mean(prob_arr, axis=0)\n","        \n","        # Compute Hamming Distance of the bitblock assignment (== # of bit errors if wrongly predicted)\n","        for i in range(len(assigned_bits)):\n","                for j in range(len(assigned_bits)):\n","                    hamming_dist = np.sum(np.bitwise_xor(assigned_bits[i], assigned_bits[j]))\n","                    bit_err.append(hamming_dist)\n","        \n","        # Convert the probability distribution of the predictions to BER with the already assigned bits\n","        bit_err = np.reshape(np.asarray(bit_err), (self.M, self.M))\n","        bit_prob = np.multiply(prob, bit_err) / (self.M*self.k) \n","        BER = np.sum(bit_prob)\n","\n","        return BER, errorsum\n","    \n","    def plot_BER(self, batch_size, ebnodbs, channel_realisations, logdir):\n","        '''Plot BER over various ebno-levels'''\n","        \n","        ti = time.time()\n","        ber = []\n","        # Initialize Logfile\n","        logmsg = 'Simulation starting time: ' + str(ti) + '\\n \\n'\n","        with open(logdir, 'w+') as logfile:\n","                        logfile.write(logmsg)\n","        \n","        # Assign bit-blocks to messages        \n","        assigned_bits = self.assign_bits_to_msg(1000, 30, 10000)\n","        \n","        # Iterate over ebno-levels and compute BER with minimum computation and termination citeria    \n","        for _, ebno in enumerate(ebnodbs):\n","            i = 1\n","            blockerror_sum = 0\n","            ber_i = []\n","            while True:\n","                ber_val, abs_blockerrors = ae.calc_BER(batch_size, ebno, channel_realisations, assigned_bits)\n","                blockerror_sum += abs_blockerrors\n","                \n","                # Minimum block errors\n","                if (0 <= ebno < 15) and i < 50 and blockerror_sum < 100:\n","                    ber_i.append(ber_val)\n","                    i += 1\n","                    \n","                elif (15 <= ebno < 20) and i < 50 and blockerror_sum < 100:\n","                    ber_i.append(ber_val)\n","                    i += 1\n","                                        \n","                elif (20 <= ebno < 27) and i < 50 and blockerror_sum < 100:\n","                    ber_i.append(ber_val)\n","                    i += 1\n","                                        \n","                elif (27 <= ebno <= 35) and i < 50 and blockerror_sum < 100:\n","                    ber_i.append(ber_val)\n","                    i += 1\n","                \n","                # Min iterations to average\n","                elif (15 <= ebno < 20) and (i < 5):\n","                    ber_i.append(ber_val)\n","                    i += 1\n","                \n","                elif (20 <= ebno < 28) and (i < 10):\n","                    ber_i.append(ber_val)\n","                    i += 1\n","                    \n","                elif (28 <= ebno <= 35) and (i < 15):\n","                    ber_i.append(ber_val)\n","                    i += 1\n","                    \n","                else:\n","                    ber_i.append(ber_val)\n","                    break\n","                    \n","            tim = time.time() - ti\n","            ber_app = np.mean(ber_i, dtype=np.float64) #np.sum(ber_i) / len(ber_i)     \n","            msg = 'EbN0: ' + str(ebno) + ' , Zeit: ' + str(round(tim/60, 2)) + 'min , Averaged over: ' + str(len(ber_i)) + \\\n","                  ', Absolute Blockerrors: ' + str(blockerror_sum) + ', BER: ' + str(np.asscalar(np.round(ber_app, 5))) + '\\n'\n","            print(msg)\n","            with open(logdir, 'a+') as logfile:\n","                logfile.write(msg)\n","            ber.append(ber_app)\n","        \n","        # Plot BER-Curve\n","        image = plt.figure(figsize=(10,8))\n","        plt.plot(ebnodbs, ber, '-r', linewidth=2.0)\n","        plt.yscale('log')\n","        plt.xlabel('EbNo (dB)', fontsize=18)\n","        plt.ylabel('avBER', fontsize=18)\n","        plt.grid(True)\n","        plt.ylim([1e-5, 1])\n","        parameter = {\"ytick.color\": \"k\", \"xtick.color\": \"k\", \"axes.labelcolor\": \"k\", \"axes.edgecolor\": \"k\"}\n","        plt.rcParams.update(parameter)\n","        plt.legend(['Autoencoder'], prop={'size': 16}, loc='upper right')\n","        \n","        return image, ber\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ljEQ90a-USWw","colab_type":"text"},"cell_type":"markdown","source":["## Trainingsloop 1 as described in Master's Thesis"]},{"metadata":{"collapsed":true,"id":"wR7B02viUSWz","colab_type":"text"},"cell_type":"markdown","source":["\n"]},{"metadata":{"id":"6Xi15GpVUSW2","colab_type":"code","colab":{}},"cell_type":"code","source":["# First define Training parameters such as Train-Ebno, Learning Rate, batchsize, validation parameters\n","train_EbNodB_arr = [3, 7, 11, 13, 17, 22, 25, 28, 32]  \n","lr = np.round(np.linspace(0.01, 0.0001, num=len(train_EbNodB_arr)), 5)\n","batch_size_tr = 1000\n","iterations = 10000\n","repeat = 1  # With that number you force to train on one EbN0 Level multiple times\n","repeats = np.rint(np.linspace(repeat, repeat, num=len(train_EbNodB_arr))).astype(np.int32)\n","val_EbNodB_arr = train_EbNodB_arr\n","\n","# Initialize Model\n","print('Initializing Graph... with M = ' + str(2**k))\n","ae = AE(k, n, mt, mr, seed)\n","print('Initialized... \\n ')\n","\n","# Start training loop\n","t = time.time()\n","for ind, ebno in enumerate(train_EbNodB_arr, 1):\n","    train_EbNodB = ebno\n","    val_EbNodB = ebno\n","    print('\\nTraining with ' + str(ebno) + ' EbN0 and lr = ' + str(lr[ind-1]))\n","    training_params = [\n","    # batch_size, lr, ebnodb, iterations, repeats\n","    [batch_size_tr, lr[ind-1], train_EbNodB, iterations, repeats[ind-1]]]\n","    validation_params = [\n","    # batch_size, ebnodb, val_steps\n","    [(100*batch_size_tr), val_EbNodB, (iterations/10)]]\n","\n","    ae.train(training_params, validation_params)\n","     \n","elapsed = time.time() - t\n","print('\\n Done. Training time: ' + str(round(elapsed, 2)) + ' sec bzw. ' + str(round(elapsed/60, 2)) + ' min')\n","\n","# Save Model\n","model_file = model_file_1 + 'models/AE' + model_file_2\n","ae.save(model_file)\n","print('\\nModel saved.')\n"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"hvK9-RYfUSW-","colab_type":"text"},"cell_type":"markdown","source":["## Trainingsloop 2 (Other possibility, not described in Master's Thesis and not really used)\n","\n","Difference: Variable batchsize and iterations\n"]},{"metadata":{"id":"ILQts4y9USXA","colab_type":"code","colab":{}},"cell_type":"code","source":["# Define Training parameters\n","train_EbNodB_arr = [20] # [3, ..., 25]\n","lr = np.logspace(-1, -5, 5)\n","iterations = [100, 100, 1000, 1000, 1000]\n","batch_size_tr = 1000\n","repeats_float = np.rint(np.linspace(1, 3, num=len(train_EbNodB_arr)))\n","repeats = np.flip(repeats_float.astype(np.int32), axis=0)\n","val_EbNodB_arr = train_EbNodB_arr\n","\n","# Initialize Model\n","print('Initializing Graph... with M = ' + str(2**k))\n","ae = AE(k, n, seed)\n","print('Initialized... \\n ')\n","\n","# Start Training Loop\n","t = time.time()\n","for lr_i in range(len(lr)):    \n","    for ind, ebno in enumerate(train_EbNodB_arr, 1):\n","        train_EbNodB = ebno\n","        val_EbNodB = ebno\n","        print('\\nTraining with ' + str(ebno) + ' EbN0 and lr = ' + str(lr[lr_i]))\n","        training_params = [\n","        # batch_size, lr, ebnodb, iterations, repeats\n","        [batch_size_tr, lr[lr_i], train_EbNodB, iterations[lr_i], repeats[ind-1]]]\n","        validation_params = [\n","        # batch_size, ebnodb, val_steps\n","        [(100*batch_size_tr), val_EbNodB, (iterations[lr_i]/10)]]\n","    \n","        ae.train(training_params, validation_params)\n","     \n","elapsed = time.time() - t\n","print('\\n Done. Training time: ' + str(round(elapsed, 2)) + ' sec bzw. ' + str(round(elapsed/60, 2)) + ' min')\n","\n","# Save Model\n","model_file = model_file_1 + 'models/AE_loop2' + model_file_2\n","ae.save(model_file)\n","print('\\nModel saved.')\n"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"s0d3m4EzUSXF","colab_type":"text"},"cell_type":"markdown","source":["## Load presaved Model\n"]},{"metadata":{"id":"yCZ7Ei_lUSXI","colab_type":"code","colab":{},"outputId":"1c6b54f9-c1a1-4e82-e054-6e31f43e5679"},"cell_type":"code","source":["model_file = model_file_1 + 'models/AE' + model_file_2\n","ae = AE(k, n, mt, mr, seed, filename=model_file) #Load a pretrained model that you have saved if needed\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from AE_SIMO/models/AE_k_2_n_1_tx_1_rx_3_s_99\n"],"name":"stdout"}]},{"metadata":{"id":"093nxcLOUSXT","colab_type":"text"},"cell_type":"markdown","source":["## Plot learned symbolconstellation"]},{"metadata":{"collapsed":true,"id":"gzGuPIg1USXV","colab_type":"text"},"cell_type":"markdown","source":["\n"]},{"metadata":{"id":"uMFildFDUSXX","colab_type":"code","colab":{}},"cell_type":"code","source":["_, image = ae.plot_constellation()\n","plotfile = model_file_1 + 'Pics/const_AE' + model_file_2 + '.pdf'\n","plt.savefig(plotfile, bbox_inches='tight')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e-WgGPkfUSXc","colab_type":"text"},"cell_type":"markdown","source":["## Plot received Symbols"]},{"metadata":{"collapsed":true,"id":"ueP1VMGFUSXf","colab_type":"text"},"cell_type":"markdown","source":["\n"]},{"metadata":{"id":"_bVCYbqqUSXh","colab_type":"code","colab":{}},"cell_type":"code","source":["rtn = 1  # 0 for received symbols and 1 if after Predecoder-NN\n","ebno = 20\n","numofmsgrepeats= 50\n","_,x = ae.plot_receive(ebno, numofmsgrepeats, rtn) \n","plotfile = model_file_1 + 'Pics/final/rx_const_AE' + model_file_2 + '_rtn_{}.pdf'.format(rtn)\n","plt.savefig(plotfile, bbox_inches='tight')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"--gTUmH6USXn","colab_type":"text"},"cell_type":"markdown","source":["## BLER Simulations"]},{"metadata":{"collapsed":true,"id":"s4TfbF5oUSXp","colab_type":"text"},"cell_type":"markdown","source":["\n"]},{"metadata":{"id":"fycjOv-SUSXs","colab_type":"code","colab":{}},"cell_type":"code","source":["batch = 100\n","channel_realizations = 10000\n","ti = time.time()\n","\n","# EbN0 Vector\n","ebnodbs = np.linspace(0, 35, 36)\n","\n","# Simulation\n","print('Starting Simulation... \\n')\n","blers = ae.bler_sim(ebnodbs, batch, channel_realizations)\n","ae.plot_bler(ebnodbs, blers)\n","elapsed_1 = time.time() - ti\n","print('\\n Done. Simulation time: ' + str(round(elapsed_1, 2)) + ' sec bzw. ' + str(round(elapsed_1/60, 2)) + ' min')\n","\n","# Save Fig\n","plotfile = model_file_1 + 'Pics/bler_ae' + model_file_2 + '.png'\n","plt.savefig(plotfile)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9xwNvpqBUSXw","colab_type":"text"},"cell_type":"markdown","source":["## BER Simulations"]},{"metadata":{"collapsed":true,"id":"h7JITzpwUSXx","colab_type":"text"},"cell_type":"markdown","source":["\n"]},{"metadata":{"id":"lzRDD-tTUSXz","colab_type":"code","colab":{}},"cell_type":"code","source":["batch = 1000\n","channel_realizations = 10000\n","logfile = model_file_1 + 'Pickle/LOGDATA_BER_ae' + model_file_2 + '.txt'\n","ti = time.time()\n","\n","# EbN0 Vector\n","ebnodbs = np.linspace(0, 35, 36)\n","\n","# Simulation\n","print('Starting Simulation... \\n')\n","_, ber = ae.plot_BER(batch, ebnodbs, channel_realizations, logfile)\n","elapsed_1 = time.time() - ti\n","print('\\n Done. Simulation time: ' + str(round(elapsed_1, 2)) + ' sec bzw. ' + str(round(elapsed_1/60, 2)) + ' min')\n","\n","# Save Fig and BER-Vector in Files\n","plotfile = model_file_1 + 'Pics/BER_ae' + model_file_2 + '.png'\n","plt.savefig(plotfile)\n","\n","pick_file = model_file_1 + 'Pickle/BER_ae' + model_file_2 + '.pckl'\n","f = open(pick_file, 'wb')\n","pickle.dump(ber, f)\n","f.close()"],"execution_count":0,"outputs":[]}]}