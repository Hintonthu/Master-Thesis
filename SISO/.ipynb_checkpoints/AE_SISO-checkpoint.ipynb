{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SISO model - Training an end-to-end communications system on a Rayleigh-Fading channel based on an autoencoder model\n",
    "\n",
    "This model is modified explicitly for the SISO scenario\n",
    "\n",
    "Hint: To prevent memory garbage on disk suppress summary generation when not explicitly desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "import pickle\n",
    "import itertools as it\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Ignore if you do not have multiple GPUs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section defines the parameters which need to be fed to the model for the initializiation process\n",
    "\n",
    "- Be aware that the parameters influence the size of the neural networks\n",
    "- k is the number of information bits per message, note that M = 2**k\n",
    "- n is the number of complex channel uses per message, this parameter handles the channel coding --> a k bit message will be encoded into n consecutive complex symbols\n",
    "- seed can be used to reproduce identical results by fixing the seed of the random number generators, note that the line \"tf.set_random_seed(self.seed)\" should be commented out to set the seed in the graph, otherwise 'seed' is only used to distinguish between trained models in the saving process\n",
    "- mt and mr are the number of transmit and recive antenna respectively, in SISO they are fixed to one\n",
    "- model_file_1 and model_file_2 are only declared to distinguish models in saving processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2  # Number of information bits per message, i.e., M=2**k\n",
    "n = 1  # Number of complex channel uses per message\n",
    "seed = 11  # Seed RNG reproduce identical results\n",
    "mt = 1\n",
    "mr = 1\n",
    "model_file_1 = 'SISO/' \n",
    "model_file_2 = '_k_{}_n_{}_tx_{}_rx_{}_s_{}'.format(k, n, mt, mr, seed)\n",
    "\n",
    "assert (mt, mr) == (1, 1), \"Number of Transmit or Receive antennas does not equal 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Autoencoder with subfunctions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(object):\n",
    "    def __init__(self, k, n, seed=None, filename=None):\n",
    "        '''This function is the initialization function of the Autoencoder class'''\n",
    "        self.k = k \n",
    "        self.n = n\n",
    "        self.bits_per_symbol = self.k/self.n\n",
    "        self.M = 2**self.k\n",
    "        self.seed = seed if (seed is not None) else int(time.time())           \n",
    "        self.graph = None\n",
    "        self.sess = None   \n",
    "        self.vars = None\n",
    "        self.saver = None   \n",
    "        self.constellations = None\n",
    "        self.blers = None\n",
    "        self.create_graph()\n",
    "        self.create_session()\n",
    "        self.sum = self.sum_writer()\n",
    "        if filename is not None:    \n",
    "            self.load(filename)       \n",
    "        return\n",
    "    \n",
    "    def create_graph(self):\n",
    "        '''This function creates the computation graph of the autoencoder'''\n",
    "        self.graph = tf.Graph()        \n",
    "        with self.graph.as_default():    \n",
    "            # Comment out if you want to set the RNG seed\n",
    "            #tf.set_random_seed(self.seed)\n",
    "            \n",
    "            # Placeholder which defines the input dimension of the AE\n",
    "            batch_size = tf.placeholder(tf.int32, shape=())\n",
    "            \n",
    "            # Create Channelmatrix\n",
    "            hc, h_resh = self.channel_matrix()\n",
    "            \n",
    "            # Transmitter Functions\n",
    "            s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64) # Uniformly distributed messages of M messages\n",
    "            x, x_plot, vars_enc = self.encoder(s)     \n",
    "            \n",
    "            # Channel\n",
    "            noise_std = tf.placeholder(tf.float32, shape=())\n",
    "            y, y_plot = self.channel(x, hc, noise_std)\n",
    "            \n",
    "            # Receiver\n",
    "            s_hat, vars_dec = self.decoder(y, h_resh)\n",
    "            \n",
    "            # Loss function\n",
    "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
    "                \n",
    "            # Performance metrics\n",
    "            preds = tf.nn.softmax(s_hat)\n",
    "            s_hat_bit = tf.argmax(preds, axis=1)\n",
    "            correct_predictions = tf.equal(s_hat_bit, s)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "            bler = 1 - accuracy\n",
    "    \n",
    "            # Optimizer\n",
    "            lr = tf.placeholder(tf.float32, shape=())  \n",
    "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "                       \n",
    "            # Code lines which need to be implemented to monitor AE behavior through Tensorboard \n",
    "            with tf.name_scope('performance'):\n",
    "                tf.summary.scalar('loss', cross_entropy)\n",
    "                tf.summary.scalar('bler', bler)\n",
    "            \n",
    "            # Handle Training Parameters to monitor them\n",
    "            train_vars = tf.trainable_variables()  \n",
    "            train_v = [vars_enc, vars_dec]\n",
    "         \n",
    "            merged = tf.summary.merge_all()\n",
    "        \n",
    "            # References to graph variables we need to access later \n",
    "            self.vars = {\n",
    "                'accuracy': accuracy,\n",
    "                'batch_size': batch_size,\n",
    "                'bler': bler,\n",
    "                'cross_entropy': cross_entropy,\n",
    "                'init': tf.global_variables_initializer(),\n",
    "                'lr': lr,\n",
    "                'noise_std': noise_std,\n",
    "                'train_op': train_op,\n",
    "                's': s,\n",
    "                'h': hc,\n",
    "                'y': y,\n",
    "                's_hat': s_hat,\n",
    "                'x': x,\n",
    "                'x_plot': x_plot,\n",
    "                'y_plot': y_plot,\n",
    "                'merged': merged,\n",
    "                'train_vars': train_vars,\n",
    "                'train_v': train_v,\n",
    "                'preds': preds,\n",
    "                'corr_preds': correct_predictions\n",
    "            }            \n",
    "            self.saver = tf.train.Saver()\n",
    "        return\n",
    "    \n",
    "    def create_session(self):\n",
    "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
    "        self.sess = tf.Session(graph=self.graph)        \n",
    "        self.sess.run(self.vars['init'])\n",
    "        return\n",
    "    \n",
    "    def channel_matrix(self):\n",
    "        '''The Channel Matrix Calculation (Rayleigh Fading Channel)'''\n",
    "        \n",
    "        # Define standard deviation as Rayleigh\n",
    "        h_std_r = np.sqrt(0.5)\n",
    "        h_mean_r = 0\n",
    "        h_std_i = np.sqrt(0.5)\n",
    "        h_mean_i = 0\n",
    "        \n",
    "        # Calculate channel symbols\n",
    "        hr = tf.random_normal([1, 1], mean=h_mean_r, stddev=h_std_r, dtype=tf.float32)\n",
    "        hi = tf.random_normal([1, 1], mean=h_mean_i, stddev=h_std_i, dtype=tf.float32)\n",
    "        hc = tf.complex(hr, hi)\n",
    "        \n",
    "        # Channel in real notation\n",
    "        h = tf.concat([tf.real(hc), tf.imag(hc)], axis=1)\n",
    "        h_resh = tf.reshape(h, shape=[1, -1])\n",
    "        \n",
    "        # Channel in complex notation\n",
    "        hc = tf.reshape(hc, shape=[])\n",
    "\n",
    "        return hc, h_resh\n",
    "    \n",
    "    def encoder(self, input):\n",
    "        '''The transmitter (Encoder-NN)'''\n",
    "        \n",
    "        # Embedding Layer  \n",
    "        with tf.name_scope('enc_1'):\n",
    "            # Embedding Matrix\n",
    "            W = self.weight_variable((self.M, self.M))\n",
    "            # Embedding Look-Up and ELU-Activation\n",
    "            x = tf.nn.elu(tf.nn.embedding_lookup(W, input, name='enc_1'))\n",
    "            # Store layer parameters to monitor them in Tensorboard\n",
    "            var_enc_1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'enc_1')\n",
    "            \n",
    "        # Hidden Layer\n",
    "        with tf.name_scope('enc_2'):\n",
    "            # Dense Layer\n",
    "            x = tf.layers.dense(x, 2*self.n, activation=tf.nn.relu , name='enc_2')\n",
    "            # Store layer parameters to monitor them in Tensorboard\n",
    "            var_enc_2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'enc_2')\n",
    "            \n",
    "        # Output Layer\n",
    "        with tf.name_scope('enc_3'):\n",
    "            # Dense Layer\n",
    "            x = tf.layers.dense(x, 2*self.n, activation=None, name='enc_3')\n",
    "            # Store layer parameters to monitor them in Tensorboard\n",
    "            var_enc_3 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'enc_3')\n",
    "        \n",
    "        # Concat all summaries of the layer parameters\n",
    "        vars = [var_enc_1, var_enc_2, var_enc_3]\n",
    "        \n",
    "        # Reshape to actual Batchsize where one row represent one message        \n",
    "        x = tf.reshape(x, shape=[-1, 2, self.n])\n",
    "        \n",
    "        # Real to complex transformation\n",
    "        x = tf.reshape(x, shape=[-1, 2])\n",
    "        x = tf.complex(x[:, 0], x[:, 1])\n",
    "        \n",
    "        # Power Normalization (Comment out the one you want to use)\n",
    "        '''\n",
    "        # Average power normalization\n",
    "        norma = tf.sqrt(tf.reduce_mean(tf.square(tf.abs(x))))\n",
    "        x = x / tf.cast(norma, dtype=tf.complex64) '''\n",
    "        \n",
    "        # Power Normalization = 1\n",
    "        norma = tf.reciprocal(tf.sqrt(tf.square(tf.abs(x))))\n",
    "        x = tf.multiply(x, tf.cast(norma, dtype=tf.complex64))\n",
    "\n",
    "        # x_plot is reshaped for plotting purposes                \n",
    "        x_plot = tf.reshape(x, shape=[-1, self.n])\n",
    "        \n",
    "        return x, x_plot, vars\n",
    "    \n",
    "    def channel(self, input, hc, noise_std):\n",
    "        '''The channel (Rayleigh-Fading Channel)''' \n",
    "        \n",
    "        # Channel Matrix Multiplicatipon\n",
    "        yc = tf.scalar_mul(hc, input)\n",
    "        \n",
    "        # Noise\n",
    "        nr = tf.random_normal(tf.shape(yc), mean=0.0, stddev=(noise_std/np.sqrt(2)), dtype=tf.float32)\n",
    "        ni = tf.random_normal(tf.shape(yc), mean=0.0, stddev=(noise_std/np.sqrt(2)), dtype=tf.float32)\n",
    "        nc = tf.complex(nr, ni)\n",
    "        yc = tf.add(yc, nc)\n",
    "        \n",
    "        # y_plot just here if you need to modify the plotting input\n",
    "        y_plot = yc\n",
    "        \n",
    "        return yc, y_plot\n",
    "    \n",
    "    def decoder(self, yc, h):\n",
    "        '''The Receiver (Decoder-NN='''\n",
    "        \n",
    "        # Complex to real\n",
    "        yc = tf.reshape(yc, shape=[-1, 1])\n",
    "        y = tf.concat([tf.real(yc), tf.imag(yc)], axis=1)\n",
    "        \n",
    "        # Reshape: one row of matrix is assciated to one message\n",
    "        y = tf.reshape(y, shape=[-1, 2*self.n])\n",
    "        \n",
    "        # Concat CSIR to y\n",
    "        h = tf.tile(h, [tf.shape(y)[0], 1])\n",
    "        input = tf.concat([y, h], 1)\n",
    "        \n",
    "        # Input Layer\n",
    "        with tf.name_scope('dec_1'):\n",
    "            # Dense Layer\n",
    "            y = tf.layers.dense(input, 2*self.M, activation=tf.nn.relu, name='dec_1')\n",
    "            # Store layer parameters to monitor them in Tensorboard\n",
    "            var_dec_1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'dec_1')\n",
    "            \n",
    "        # Hidden Layer\n",
    "        with tf.name_scope('dec_2'):\n",
    "            # Dense Layer\n",
    "            y = tf.layers.dense(y, self.M, activation=tf.nn.relu, name='dec_2')\n",
    "            # Store layer parameters to monitor them in Tensorboard\n",
    "            var_dec_2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'dec_2')\n",
    "            \n",
    "        # Output Layer\n",
    "        with tf.name_scope('dec_3'):\n",
    "            # Dense Layer\n",
    "            y = tf.layers.dense(y, self.M, activation=None, name='dec_3')\n",
    "            # Store layer parameters to monitor them in Tensorboard\n",
    "            var_dec_3 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'dec_3')\n",
    "        \n",
    "        # Concat all summaries of the layer parameters    \n",
    "        vars = [var_dec_1, var_dec_2, var_dec_3]\n",
    "        \n",
    "        # Output is P due to softmax-activation      \n",
    "        return y, vars\n",
    "    \n",
    "    def EbNo2Sigma(self, ebnodb):\n",
    "        '''Convert Eb/No in dB to noise standard deviation'''\n",
    "        ebno = 10**(ebnodb/10)\n",
    "        return np.sqrt(1 / (self.bits_per_symbol * ebno))\n",
    "    \n",
    "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
    "        '''Generate a feed dictionary for training and validation'''        \n",
    "        return {\n",
    "            self.vars['batch_size']: batch_size,\n",
    "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
    "            self.vars['lr']: lr,\n",
    "        }    \n",
    "    \n",
    "    def load(self, filename):\n",
    "        '''Load an pre_trained model'''\n",
    "        return self.saver.restore(self.sess, filename)\n",
    "        \n",
    "    def plot_constellation(self, maxrange=None):\n",
    "        '''Generate a plot of the current constellation'''\n",
    "        \n",
    "        # Get symbols \n",
    "        x = np.reshape(self.transmit(range(self.M)), (self.M, self.n))\n",
    "        \n",
    "        # Define plot parameters\n",
    "        if (maxrange is None):\n",
    "            maxrange = np.max(np.abs(x))\n",
    "        print('Constellation points: ' + str(x))\n",
    "        colours = cm.cmap_d['gist_rainbow'](np.linspace(0, 1, self.M)) \n",
    "        image = plt.figure(figsize=(6, 6))\n",
    "        plt.grid(True)\n",
    "        plt.xlim(-maxrange,maxrange)\n",
    "        plt.ylim(-maxrange,maxrange)\n",
    "        \n",
    "        # Calculate bit-block assignement\n",
    "        bit_assigned = self.assign_bits_to_msg(100, 30, 10000)\n",
    "        \n",
    "        # Plot symbols\n",
    "        for j in range(self.M):\n",
    "            for k in range(self.n):\n",
    "                plt.scatter(x[j, k].real, x[j, k].imag, c=colours[j], label=bit_assigned[j] ,marker='x', s=75)\n",
    "                if self.n > 1:\n",
    "                    plt.annotate(k, (x[j, k].real+0.05, x[j, k].imag+0.05))\n",
    "                    \n",
    "        plt.legend(bit_assigned, loc='upper center', bbox_to_anchor=(1.2,1), ncol=1, fontsize=16, handletextpad=0.05, labelspacing=0.2)\n",
    "        parameter = {\"ytick.color\": \"k\", \"xtick.color\": \"k\", \"axes.labelcolor\": \"k\", \"axes.edgecolor\": \"k\"}\n",
    "        plt.xlabel('I', fontsize=18)\n",
    "        plt.ylabel('Q', fontsize=18)\n",
    "        plt.tick_params(labelsize=16)\n",
    "        plt.rcParams.update(parameter)\n",
    "        image.axes[0].set_xticks(np.array([-2, -1, 0, 1, 2]))                                          \n",
    "        image.axes[0].set_yticks(np.array([-2, -1, 0, 1, 2]))\n",
    "        return x, image\n",
    "    \n",
    "    def plot_receive(self, ebn0, channel_realizations, maxrange=None):\n",
    "        '''Generate a plot of the current constellation'''\n",
    "        \n",
    "        # Get symbols\n",
    "        y = self.receive(ebn0, channel_realizations)\n",
    "        \n",
    "        # Define plot parameters\n",
    "        if (maxrange is None):\n",
    "            maxrange = np.max(np.abs(y))\n",
    "        colours = cm.cmap_d['gist_rainbow'](np.linspace(0, 1, self.M)) \n",
    "        image = plt.figure(figsize=(6, 6))\n",
    "        plt.grid(True)\n",
    "        plt.xlim(-maxrange,maxrange)\n",
    "        plt.ylim(-maxrange,maxrange)\n",
    "        \n",
    "        # Calculate bit assignment\n",
    "        bit_assigned = self.assign_bits_to_msg(100,30,10000)\n",
    "        \n",
    "        # Plot symbols\n",
    "        for j in range(channel_realizations):\n",
    "            symbols = y[j]\n",
    "            for k in range(self.M):\n",
    "                plt.scatter(symbols[k].real, symbols[k].imag, marker='x', c=colours[k], s=75)\n",
    "\n",
    "        plt.legend(bit_assigned, loc='upper center', bbox_to_anchor=(1.2,1), ncol=1, fontsize=16, handletextpad=0.05, labelspacing=0.2)\n",
    "        parameter = {\"ytick.color\": \"k\", \"xtick.color\": \"k\", \"axes.labelcolor\": \"k\", \"axes.edgecolor\": \"k\"}\n",
    "        plt.xlabel('I', fontsize=18)\n",
    "        plt.ylabel('Q', fontsize=18)\n",
    "        plt.locator_params(nbins=5)\n",
    "        plt.tick_params(labelsize=16)\n",
    "        plt.rcParams.update(parameter)\n",
    "        return y, image\n",
    "    \n",
    "    def save(self, filename):\n",
    "        '''Save the current model'''\n",
    "        return self.saver.save(self.sess, filename)  \n",
    "    \n",
    "    def test_step(self, batch_size, ebnodb):\n",
    "        '''Compute the BLER over a single batch and Eb/No'''\n",
    "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
    "        return bler\n",
    "    \n",
    "    def transmit(self, s):\n",
    "        '''Returns the transmitted signals corresponding to message indices (BE AWARE: RETURNS x_plot and not x!)'''\n",
    "        return self.sess.run(self.vars['x_plot'], feed_dict={self.vars['s']: s})\n",
    "    \n",
    "    def receive(self, ebnodb, channel_realizations):\n",
    "        '''Returns the received signals of multiple sending of the M consecutive messages (BE AWARE: Uses y_plot and not y!)\n",
    "                            Returns: Array of M x channel_realizations'''\n",
    "        y_ch = []\n",
    "        s = range(self.M)\n",
    "        noise_std = self.EbNo2Sigma(ebnodb)\n",
    "        for i in range(channel_realizations):\n",
    "            y = self.sess.run(self.vars['y_plot'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n",
    "            y_ch.append(y)        \n",
    "        \n",
    "        return y_ch\n",
    "    \n",
    "    def train(self, training_params, validation_params):\n",
    "        '''Main training function, which handles train and validation paramters'''\n",
    "        train_bler = []\n",
    "        valid_bler = []\n",
    "        itera = 1\n",
    "        \n",
    "        # Training and validation loop\n",
    "        for index, params in enumerate(training_params):            \n",
    "            batch_size, lr, ebnodb, iterations, repeats = params            \n",
    "            print('\\nBatch Size: ' + str(batch_size) +\n",
    "                  ', Learning Rate: ' + str(lr) +\n",
    "                  ', EbNodB: ' + str(ebnodb) +\n",
    "                  ', Iterations: ' + str(iterations) +\n",
    "                  ', Repeats: ' + str(repeats))\n",
    "            \n",
    "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
    "            \n",
    "            for j in range(repeats):          \n",
    "                for i in range(iterations):\n",
    "                    \n",
    "                    # Call training function\n",
    "                    summary, tr_bler = self.train_step(batch_size, ebnodb, lr)\n",
    "                    \n",
    "                    # Observe train and val results\n",
    "                    train_bler.append([itera, tr_bler])\n",
    "                    self.sum.add_summary(summary, i)\n",
    "                    if i%val_steps==0 and index==0 and j==0:\n",
    "                        val_bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
    "                        print(val_bler)\n",
    "                        valid_bler.append([itera, val_bler])\n",
    "                    itera += 1\n",
    "           \n",
    "            bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
    "            print(bler)\n",
    "            \n",
    "        return train_bler, valid_bler\n",
    "    \n",
    "    def train_step(self, batch_size, ebnodb, lr):\n",
    "        '''A single training step: Computes one iteration of the graph and  optimizes neural net paramters'''\n",
    "        summary, _, train_bler = self.sess.run([self.vars['merged'], self.vars['train_op'], self.vars['bler']],\n",
    "                                               feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
    "        return summary, train_bler\n",
    "    \n",
    "    def sum_writer(self):\n",
    "        '''Summary writer function which is needed for Tensorboard visualization'''\n",
    "        sum_writer = tf.summary.FileWriter(model_file_1 + 'Summary', self.sess.graph)\n",
    "        return sum_writer\n",
    "    \n",
    "    def get_graphkeys(self):\n",
    "        '''Get the trained weights and biases of the layers'''\n",
    "        theta = self.sess.run(self.vars['train_v'])\n",
    "        return theta\n",
    "    \n",
    "    def weight_variable(self, shape):\n",
    "        '''Xavier-initialized weights in Embedding Matrix'''\n",
    "        (fan_in, fan_out) = shape\n",
    "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
    "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
    "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
    "    \n",
    "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
    "        '''Monte Carlo simulations of BLER for a range of Eb/No, averaged over iterations with one iteration having size batch_size'''\n",
    "        BLER = np.zeros_like(ebnodbs)\n",
    "        for i in range(iterations):\n",
    "            bler = np.array([self.sess.run(self.vars['bler'],\n",
    "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
    "            BLER = BLER + bler/iterations\n",
    "        return BLER\n",
    "    \n",
    "    def plot_bler(self, EbNodB, BLER):\n",
    "        '''Plot a BLER curve'''\n",
    "        image = plt.figure(figsize=(10,8))\n",
    "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
    "        plt.ylabel('Block-error rate', fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.ylim([1e-5, 1])\n",
    "        parameter = {\"ytick.color\": \"k\", \"xtick.color\": \"k\", \"axes.labelcolor\": \"k\", \"axes.edgecolor\": \"k\"}\n",
    "        plt.rcParams.update(parameter)\n",
    "        plt.legend(['Autoencoder'], prop={'size': 16}, loc='upper right')\n",
    "        return image\n",
    "        \n",
    "    def assign_bits_to_msg(self, batch_size, ebnodb, channel_realisations):\n",
    "        '''Compute bit assignment to message indices: Get average BER over batchsize on multiple channelrealizations \n",
    "                for specific EbN0; Take the bit assignment which produces lowest BER'''\n",
    "        \n",
    "        prob_arr = []\n",
    "        # Average the prediction probabilities for each message when decoded over a loop\n",
    "        for i in range(channel_realisations):            \n",
    "            prob = np.zeros((self.M, self.M))\n",
    "            s, preds = self.sess.run([self.vars['s'], self.vars['preds']], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
    "            \n",
    "            for ind, msg in enumerate(s):\n",
    "                prob[msg] = prob[msg] + preds[ind]\n",
    "                \n",
    "            unique, counts = np.unique(s, return_counts=True)\n",
    "            occurence = dict(zip(unique, counts))           \n",
    "        \n",
    "            for msg, cnts in occurence.items():\n",
    "                if (cnts == 0):\n",
    "                    prob[msg] = prob[msg]\n",
    "                else:\n",
    "                    prob[msg] = prob[msg] / cnts\n",
    "                    \n",
    "            prob_arr.append(prob)\n",
    "            \n",
    "        prob = np.mean(prob_arr, axis=0)\n",
    "        \n",
    "        # Try out all bit assignments possible for the bitblocks and compute BER\n",
    "        bit_one = []\n",
    "        bit_list = []\n",
    "        msg_list = list(map(list, it.product([0, 1], repeat=self.k)))\n",
    "        \n",
    "        for k in range(len(msg_list)):\n",
    "            msg_list = np.roll(msg_list, k)\n",
    "            bit_list.append(msg_list)\n",
    "            bit_err = []\n",
    "            # Compute Hamming Distance of the bitblock assignment (== # of bit errors if wrongly predicted)\n",
    "            for i in range(len(msg_list)):\n",
    "                for j in range(len(msg_list)):\n",
    "                    hamming_dist = np.sum(np.bitwise_xor(msg_list[i], msg_list[j]))\n",
    "                    bit_err.append(hamming_dist)\n",
    "            \n",
    "            # Calculate BER with hamming dist matrix and prediction probabilities           \n",
    "            bit_err = np.reshape(np.asarray(bit_err), (self.M, self.M))\n",
    "            bit_prob = np.multiply(prob, bit_err) / (self.M*self.k)\n",
    "            bit_one.append(np.mean(bit_prob))\n",
    "        \n",
    "        # Check for the assigmnent with lowest BER\n",
    "        index_min_BER = np.argmin(bit_one)\n",
    "        assigned_bits = bit_list[int(index_min_BER)]\n",
    "               \n",
    "        return assigned_bits\n",
    "    \n",
    "    def calc_BER(self, batch_size, ebnodb, channel_realisations, assigned_bits):\n",
    "        '''Calculate average BER over a ebno-level with batchsize messages per iterations \n",
    "                            and over # of channel realizations (Monte-Carlo Simulation)'''\n",
    "               \n",
    "        bit_err = []\n",
    "        prob_arr = []\n",
    "        errorsum = 0\n",
    "        \n",
    "        # Average the prediction probabilities for each message when decoded over a loop\n",
    "        for i in range(channel_realisations):            \n",
    "            prob = np.zeros((self.M, self.M))\n",
    "            \n",
    "            # Run graph and get predictions\n",
    "            s, preds, corr_preds = self.sess.run([self.vars['s'], self.vars['preds'], self.vars['corr_preds']], \n",
    "                                     feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
    "            \n",
    "            for ind, msg in enumerate(s):\n",
    "                prob[msg] = prob[msg] + preds[ind]\n",
    "                \n",
    "            unique, counts = np.unique(s, return_counts=True)\n",
    "            occurence = dict(zip(unique, counts))           \n",
    "        \n",
    "            for msg, cnts in occurence.items():\n",
    "                if (cnts == 0):\n",
    "                    prob[msg] = prob[msg]\n",
    "                else:\n",
    "                    prob[msg] = prob[msg] / cnts\n",
    "                    \n",
    "            prob_arr.append(prob)\n",
    "            \n",
    "            # Count absolute message errors\n",
    "            block_errors_batch = batch_size - np.sum(corr_preds)\n",
    "            errorsum += block_errors_batch\n",
    "            \n",
    "        prob = np.mean(prob_arr, axis=0)\n",
    "        \n",
    "        # Compute Hamming Distance of the bitblock assignment (== # of bit errors if wrongly predicted)\n",
    "        for i in range(len(assigned_bits)):\n",
    "                for j in range(len(assigned_bits)):\n",
    "                    hamming_dist = np.sum(np.bitwise_xor(assigned_bits[i], assigned_bits[j]))\n",
    "                    bit_err.append(hamming_dist)\n",
    "        \n",
    "        # Convert the probability distribution of the predictions to BER with the already assigned bits\n",
    "        bit_err = np.reshape(np.asarray(bit_err), (self.M, self.M))\n",
    "        bit_prob = np.multiply(prob, bit_err) / (self.M*self.k) \n",
    "        BER = np.sum(bit_prob)\n",
    "\n",
    "        return BER, errorsum\n",
    "    \n",
    "    def plot_BER(self, batch_size, ebnodbs, channel_realisations, logdir):\n",
    "        '''Plot BER over various ebno-levels'''\n",
    "        \n",
    "        ti = time.time()\n",
    "        ber = []\n",
    "        # Initialize Logfile\n",
    "        logmsg = 'Simulation starting time: ' + str(ti) + '\\n \\n'\n",
    "        with open(logdir, 'w+') as logfile:\n",
    "                        logfile.write(logmsg)\n",
    "        \n",
    "        # Assign bit-blocks to messages    \n",
    "        assigned_bits = self.assign_bits_to_msg(1000, 30, 10000)\n",
    "        \n",
    "        # Iterate over ebno-levels and compute BER with minimum computation and termination citeria\n",
    "        for _, ebno in enumerate(ebnodbs):\n",
    "            i = 1\n",
    "            blockerror_sum = 0\n",
    "            ber_i = []\n",
    "            while True:\n",
    "                ber_val, abs_blockerrors = ae.calc_BER(batch_size, ebno, channel_realisations, assigned_bits)\n",
    "                blockerror_sum += abs_blockerrors\n",
    "                \n",
    "                # Minimum block errors\n",
    "                if (0 <= ebno < 15) and i < 50 and blockerror_sum < 100:\n",
    "                    ber_i.append(ber_val)\n",
    "                    i += 1\n",
    "                    \n",
    "                elif (15 <= ebno < 20) and i < 50 and blockerror_sum < 100:\n",
    "                    ber_i.append(ber_val)\n",
    "                    i += 1\n",
    "                                        \n",
    "                elif (20 <= ebno < 27) and i < 50 and blockerror_sum < 100:\n",
    "                    ber_i.append(ber_val)\n",
    "                    i += 1\n",
    "                                        \n",
    "                elif (27 <= ebno <= 35) and i < 50 and blockerror_sum < 100:\n",
    "                    ber_i.append(ber_val)\n",
    "                    i += 1\n",
    "                \n",
    "                # Min iterations to average\n",
    "                elif (15 <= ebno < 20) and (i < 5):\n",
    "                    ber_i.append(ber_val)\n",
    "                    i += 1\n",
    "                \n",
    "                elif (20 <= ebno < 28) and (i < 10):\n",
    "                    ber_i.append(ber_val)\n",
    "                    i += 1\n",
    "                    \n",
    "                elif (28 <= ebno <= 35) and (i < 15):\n",
    "                    ber_i.append(ber_val)\n",
    "                    i += 1\n",
    "                    \n",
    "                else:\n",
    "                    ber_i.append(ber_val)\n",
    "                    break\n",
    "                    \n",
    "            tim = time.time() - ti\n",
    "            ber_app = np.mean(ber_i, dtype=np.float64)\n",
    "            msg = 'EbN0: ' + str(ebno) + ' , Zeit: ' + str(round(tim/60, 2)) + 'min , Averaged over: ' + str(len(ber_i)) + \\\n",
    "                  ', Absolute Blockerrors: ' + str(blockerror_sum) + ', BER: ' + str(np.asscalar(np.round(ber_app, 5))) + '\\n'\n",
    "            print(msg)\n",
    "            with open(logdir, 'a+') as logfile:\n",
    "                logfile.write(msg)\n",
    "            ber.append(ber_app)\n",
    "        \n",
    "        # Plot BER-Curve\n",
    "        image = plt.figure(figsize=(10,8))\n",
    "        plt.plot(ebnodbs, ber, '-r', linewidth=2.0)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('$\\frac{E_{b}}{N_{0}}$ (dB)', fontsize=18)\n",
    "        plt.ylabel('BER', fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.ylim([1e-5, 1])\n",
    "        parameter = {\"ytick.color\": \"k\", \"xtick.color\": \"k\", \"axes.labelcolor\": \"k\", \"axes.edgecolor\": \"k\"}\n",
    "        plt.rcParams.update(parameter)\n",
    "        plt.legend(['Autoencoder'], prop={'size': 16}, loc='upper right')\n",
    "        \n",
    "        return image, ber\n",
    "    \n",
    "    def sort_output_weights(self, input):\n",
    "        '''Get Layer (Weights and Bias) values and sort them to have a clean represenation which can be used to draw neural nets'''\n",
    "        wab_dic = {}\n",
    "        input = np.array(input)\n",
    "        weights_e = []\n",
    "        bias_e = []\n",
    "        weights_d = []\n",
    "        bias_d = []\n",
    "        \n",
    "        # Get and sort Layerweights for Encoder and Decoder\n",
    "        for encdec, network in enumerate(input): #  Index = 1 --> Encoder, Index = 2 --> Decoder\n",
    "            \n",
    "            for number, layer in enumerate(network):\n",
    "\n",
    "                if len(layer) > 1:\n",
    "                    if encdec == 0:\n",
    "                        weights_e.append(layer[0])\n",
    "                        bias_e.append(layer[1])\n",
    "                    if encdec == 1:\n",
    "                        weights_d.append(layer[0])\n",
    "                        bias_d.append(layer[1])\n",
    " \n",
    "        print('Enc W: ' + str(weights_e) + ' Enc B: ' + str(bias_e) + ' Dec W: ' + str(weights_d) + ' Dec B: ' + str(bias_d))\n",
    "                \n",
    "        layersnum_e = []\n",
    "        layersnum_d = []\n",
    "        \n",
    "        # Compute automatic Layer-Sizes (Amount of Neurons in each Layer)\n",
    "        \n",
    "        # Enc needs with weights because the Embedding Layer has no bias\n",
    "        for i, weights in enumerate(weights_e):\n",
    "            weights = np.array(weights)\n",
    "            shap = np.shape(weights)\n",
    "            if i == 0:\n",
    "                layersnum_e.append(shap[0])\n",
    "                layersnum_e.append(shap[1])\n",
    "            else:\n",
    "                layersnum_e.append(shap[0])\n",
    "        \n",
    "        # Dec can use bias to find out amout of Neurons        \n",
    "        for i, weights in enumerate(weights_d):\n",
    "            weights = np.array(weights)\n",
    "            shap = np.shape(weights)\n",
    "            if i == 0:\n",
    "                layersnum_d.append(shap[0])\n",
    "                layersnum_d.append(shap[1])\n",
    "            else:\n",
    "                layersnum_d.append(shap[1])\n",
    "    \n",
    "        return wab_dic, weights_e, bias_e, weights_d, bias_d, layersnum_e, layersnum_d\n",
    "    \n",
    "    def draw_neural_net(self, ax, left, right, bottom, top, layer_sizes, coefs_, intercepts_, np, plt):\n",
    "        '''Draws NN via matplotlib, helps to illustrate network flow'''\n",
    "\n",
    "        ax.set_facecolor('w')\n",
    "        n_layers = len(layer_sizes)\n",
    "        v_spacing = (top - bottom)/float(max(layer_sizes)) \n",
    "        h_spacing = (right - left)/float(len(layer_sizes) - 1) \n",
    "        # Input-Arrows\n",
    "        layer_top_0 = v_spacing*(layer_sizes[0] - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_sizes[0]):\n",
    "            plt.arrow(left-0.09, layer_top_0 - m*v_spacing, 0.05, 0,  lw =1, head_width=0.01, head_length=0.02, length_includes_head=False)\n",
    "        # Nodes\n",
    "        for n, layer_size in enumerate(layer_sizes):\n",
    "            layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "            for m in range(layer_size):\n",
    "                circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/8., color='w', ec='k', zorder=4)\n",
    "                #plt.plot(n*h_spacing + left, layer_top - m*v_spacing, 'o', mfc='w', mec='k', ls='-', markersize=40)\n",
    "        \n",
    "            # Add texts\n",
    "                if n == 0:\n",
    "                    plt.text(left-0.13, layer_top - m*v_spacing -0.01, r'$x_{'+str(m+1)+'}$', fontsize=22)\n",
    "                elif (n_layers == 3) & (n == 1):\n",
    "                    '''# plt.text(n*h_spacing + left+0.00, layer_top - m*v_spacing+ (v_spacing/8.+0.01*v_spacing), r'$H_{'+str(m+1)+'}$', fontsize=15)'''\n",
    "                elif n == n_layers -1:\n",
    "                    plt.text(n*h_spacing + left+0.10, layer_top - m*v_spacing, r'$y_{'+str(m+1)+'}$', fontsize=22)\n",
    "                ax.add_artist(circle)\n",
    "                  \n",
    "        # Bias-Nodes\n",
    "        for n, layer_size in enumerate(layer_sizes):\n",
    "            if n < n_layers -1:\n",
    "                x_bias = (n+0.5)*h_spacing + left\n",
    "                y_bias = top + 0.005\n",
    "                circle = plt.Circle((x_bias, y_bias), v_spacing/8., color='w', ec='k', zorder=4)\n",
    "                # Add texts\n",
    "                plt.text(x_bias-(v_spacing/8.+0.10*v_spacing+0.01), y_bias, r'$1$', fontsize=22)\n",
    "                ax.add_artist(circle)  \n",
    "                \n",
    "        # Edges between nodes\n",
    "        for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "            layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "            for m in range(layer_size_a):\n",
    "                \n",
    "                for o in range(layer_size_b):\n",
    "                    linwid = 0.1 + 0.7*abs(coefs_[n][m, o])\n",
    "                    if coefs_[n][m, o] > 0:\n",
    "                        colorlin = 'g'\n",
    "                    elif coefs_[n][m, o] == 0:\n",
    "                        colorlin = 'b'\n",
    "                    else:\n",
    "                        colorlin = 'r'                    \n",
    "                        \n",
    "                    line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left], [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c=colorlin, lw=linwid)\n",
    "                    ax.add_artist(line)\n",
    "                    \n",
    "                    # Append Weight Values\n",
    "                    '''\n",
    "                    xm = (n*h_spacing + left)\n",
    "                    xo = ((n + 1)*h_spacing + left)\n",
    "                    ym = (layer_top_a - m*v_spacing)\n",
    "                    yo = (layer_top_b - o*v_spacing)\n",
    "                    rot_mo_rad = np.arctan((yo-ym)/(xo-xm))\n",
    "                    rot_mo_deg = rot_mo_rad*180./np.pi\n",
    "                    xm1 = xm + (v_spacing/8.+0.05)*np.cos(rot_mo_rad)\n",
    "                    if n == 0:\n",
    "                        if yo > ym:\n",
    "                            ym1 = ym + (v_spacing/8.+0.12)*np.sin(rot_mo_rad)\n",
    "                        else:\n",
    "                            ym1 = ym + (v_spacing/8.+0.05)*np.sin(rot_mo_rad)\n",
    "                    else:\n",
    "                        if yo > ym:\n",
    "                            ym1 = ym + (v_spacing/8.+0.12)*np.sin(rot_mo_rad)\n",
    "                        else:\n",
    "                            ym1 = ym + (v_spacing/8.+0.04)*np.sin(rot_mo_rad)\n",
    "                    plt.text(xm1, ym1, str(round(coefs_[n][m, o], 4)), rotation=rot_mo_deg, fontsize=10)\n",
    "                    '''\n",
    "        # Edges between bias and nodes\n",
    "        for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            if n < n_layers-1:\n",
    "                layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "                layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "        \n",
    "            for m in range(layer_size_a):\n",
    "                x_bias = (n+0.5)*h_spacing + left\n",
    "                y_bias = top + 0.005 \n",
    "                for o in range(layer_size_b):\n",
    "                    linwid = 0.1 + 0.7*abs(intercepts_[n][o])\n",
    "                    if intercepts_[n][o] > 0:\n",
    "                        colorlin = 'g'\n",
    "                    elif intercepts_[n][o] == 0:\n",
    "                        colorlin = 'b'\n",
    "                    else:\n",
    "                        colorlin = 'r'\n",
    "                    \n",
    "                    line = plt.Line2D([x_bias, (n + 1)*h_spacing + left], [y_bias, layer_top_b - o*v_spacing], c=colorlin, lw=linwid)\n",
    "                    ax.add_artist(line)\n",
    "                    \n",
    "                    # Append biaas values\n",
    "                    '''\n",
    "                    xo = ((n + 1)*h_spacing + left)\n",
    "                    yo = (layer_top_b - o*v_spacing)\n",
    "                    rot_bo_rad = np.arctan((yo-y_bias)/(xo-x_bias))\n",
    "                    rot_bo_deg = rot_bo_rad*180./np.pi\n",
    "                    xo2 = xo - (v_spacing/8.+0.01)*np.cos(rot_bo_rad)\n",
    "                    yo2 = yo - (v_spacing/8.+0.01)*np.sin(rot_bo_rad)\n",
    "                    xo1 = xo2 - 0.05*np.cos(rot_bo_rad)\n",
    "                    yo1 = yo2 - 0.05*np.sin(rot_bo_rad)\n",
    "                    plt.text(xo1, yo1, str(round(intercepts_[n][o],4)), rotation=rot_bo_deg, fontsize=10)\n",
    "                    '''    \n",
    "                    \n",
    "        # Output-Arrows\n",
    "        layer_top_0 = v_spacing*(layer_sizes[-1] - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_sizes[-1]):\n",
    "            plt.arrow(right+0.015, layer_top_0 - m*v_spacing, 0.16*h_spacing, 0,  lw =1, head_width=0.01, head_length=0.02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsloop 1 as described in Master's Thesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First define Training parameters such as Train-Ebno, Learning Rate, batchsize, validation parameters, repeats\n",
    "train_EbNodB_arr = [3, 7, 11, 13, 17, 22, 25, 28, 32]  \n",
    "lr = np.round(np.linspace(0.01, 0.0001, num=len(train_EbNodB_arr)), 5)\n",
    "batch_size_tr = 1000\n",
    "iterations = 10000\n",
    "repeat = 1  # With that number you force to train on one EbN0 Level multiple times\n",
    "repeats = np.rint(np.linspace(repeat, repeat, num=len(train_EbNodB_arr))).astype(np.int32)\n",
    "val_EbNodB_arr = train_EbNodB_arr\n",
    "\n",
    "# Initialize Model\n",
    "print('Initializing Graph... with M = ' + str(2**k))\n",
    "ae = AE(k, n, seed)\n",
    "print('Initialized... \\n ')\n",
    "\n",
    "# Start training loop\n",
    "t = time.time()\n",
    "train_bler, val_bler = [], []\n",
    "for ind, ebno in enumerate(train_EbNodB_arr, 1):\n",
    "    train_EbNodB = ebno\n",
    "    val_EbNodB = ebno\n",
    "    print('\\nTraining with ' + str(ebno) + ' EbN0 and lr = ' + str(lr[ind-1]))\n",
    "    training_params = [\n",
    "    # batch_size, lr, ebnodb, iterations, repeats\n",
    "    [batch_size_tr, lr[ind-1], train_EbNodB, iterations, repeats[ind-1]]]\n",
    "    validation_params = [\n",
    "    # batch_size, ebnodb, val_steps\n",
    "    [(100*batch_size_tr), val_EbNodB, (iterations/10)]]\n",
    "\n",
    "    tr_bler, v_bler = ae.train(training_params, validation_params)\n",
    "    train_bler.append([ind, tr_bler])\n",
    "    val_bler.append([ind, v_bler])\n",
    "\n",
    "     \n",
    "elapsed = time.time() - t\n",
    "print('\\n Done. Training time: ' + str(round(elapsed, 2)) + ' sec bzw. ' + str(round(elapsed/60, 2)) + ' min')\n",
    "\n",
    "# Save Model\n",
    "model_file = model_file_1 + 'models/AE' + model_file_2\n",
    "ae.save(model_file)\n",
    "print('\\nModel saved.')\n",
    "\n",
    "# Save model parameters in pickle file to use them for further inspections\n",
    "'''\n",
    "pick_file = model_file_1 + 'Pickle/trainandvals_ae' + model_file_2 + '.pckl'\n",
    "f = open(pick_file, 'wb')\n",
    "pickle.dump([train_bler, val_bler], f)\n",
    "f.close()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Trainingsloop 2 (Other possibility, not described in Master's Thesis and not really used)\n",
    "\n",
    "Difference: Variable batchsize and iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Training parameters\n",
    "train_EbNodB_arr = [20] # [3, ..., 25]\n",
    "lr = np.logspace(-1, -5, 5)\n",
    "iterations = [100, 100, 1000, 1000, 1000]\n",
    "batch_size_tr = 1000\n",
    "repeats_float = np.rint(np.linspace(1, 3, num=len(train_EbNodB_arr)))\n",
    "repeats = np.flip(repeats_float.astype(np.int32), axis=0)\n",
    "val_EbNodB_arr = train_EbNodB_arr\n",
    "\n",
    "# Initialize Model\n",
    "print('Initializing Graph... with M = ' + str(2**k))\n",
    "ae = AE(k, n, seed)\n",
    "print('Initialized... \\n ')\n",
    "\n",
    "# Start Training Loop\n",
    "t = time.time()\n",
    "for lr_i in range(len(lr)):    \n",
    "    for ind, ebno in enumerate(train_EbNodB_arr, 1):\n",
    "        train_EbNodB = ebno\n",
    "        val_EbNodB = ebno\n",
    "        print('\\nTraining with ' + str(ebno) + ' EbN0 and lr = ' + str(lr[lr_i]))\n",
    "        training_params = [\n",
    "        # batch_size, lr, ebnodb, iterations, repeats\n",
    "        [batch_size_tr, lr[lr_i], train_EbNodB, iterations[lr_i], repeats[ind-1]]]\n",
    "        validation_params = [\n",
    "        # batch_size, ebnodb, val_steps\n",
    "        [(100*batch_size_tr), val_EbNodB, (iterations[lr_i]/10)]]\n",
    "    \n",
    "        ae.train(training_params, validation_params)\n",
    "     \n",
    "elapsed = time.time() - t\n",
    "print('\\n Done. Training time: ' + str(round(elapsed, 2)) + ' sec bzw. ' + str(round(elapsed/60, 2)) + ' min')\n",
    "\n",
    "# Save Model\n",
    "model_file = model_file_1 + 'models/AE_loop2' + model_file_2\n",
    "ae.save(model_file)\n",
    "print('\\nModel saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load presaved Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_file = model_file_1 + 'models/AE' + model_file_2\n",
    "ae = AE(k, n, seed, filename=model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Plot of learned constellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae.plot_constellation()\n",
    "plotfile = model_file_1 + 'Pics/const_AE_' + model_file_2 + '.pdf'\n",
    "plt.savefig(plotfile, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot received Symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotfile = model_file_1 + 'Pics/reveive_AE_' + model_file_2 + '.pdf'\n",
    "ebno = 20\n",
    "numofmsgrepeats= 50\n",
    "_,x = ae.plot_receive(ebno, numofmsgrepeats) \n",
    "plt.savefig(plotfile, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLER Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = 100\n",
    "channel_realizations = 10000\n",
    "ti = time.time()\n",
    "\n",
    "# EbN0 Vector\n",
    "ebnodbs = np.linspace(0, 35, 36)\n",
    "\n",
    "# Simulation\n",
    "print('Starting Simulation... \\n')\n",
    "blers = ae.bler_sim(ebnodbs, batch, channel_realizations)\n",
    "ae.plot_bler(ebnodbs, blers)\n",
    "elapsed_1 = time.time() - ti\n",
    "print('\\n Done. Simulation time: ' + str(round(elapsed_1, 2)) + ' sec bzw. ' + str(round(elapsed_1/60, 2)) + ' min')\n",
    "\n",
    "# Save Fig\n",
    "plotfile = model_file_1 + 'Pics/bler_ae' + model_file_2 + '.png'\n",
    "plt.savefig(plotfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## BER Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1000\n",
    "channel_realizations = 10000\n",
    "logfile = model_file_1 + 'Pickle/LOGDATA_BER_ae' + model_file_2 + '.txt'\n",
    "ti = time.time()\n",
    "\n",
    "# EbN0 Vector\n",
    "ebnodbs = np.linspace(0, 35, 36)\n",
    "\n",
    "# Simulation\n",
    "print('Starting Simulation... \\n')\n",
    "_, ber = ae.plot_BER(batch, ebnodbs, channel_realizations, logfile)\n",
    "elapsed_1 = time.time() - ti\n",
    "print('\\n Done. Simulation time: ' + str(round(elapsed_1, 2)) + ' sec bzw. ' + str(round(elapsed_1/60, 2)) + ' min')\n",
    "\n",
    "# Save Fig and BER-Vector in Files\n",
    "plotfile = model_file_1 + 'Pics/BER_ae' + model_file_2 + '.png'\n",
    "plt.savefig(plotfile)\n",
    "\n",
    "pick_file = model_file_1 + 'Pickle/BER_ae' + model_file_2 + '.pckl'\n",
    "f = open(pick_file, 'wb')\n",
    "pickle.dump(ber, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Illustrate Weights in NN seperately for Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get NN Parameters\n",
    "theta_dic, we, be, wd, bd, le, ld = ae.sort_output_weights(ae.get_graphkeys())\n",
    "\n",
    "# Encoder\n",
    "fig1 = plt.figure(figsize=(16, 16))\n",
    "ax = fig1.gca()\n",
    "ax.axis('off')\n",
    "ax.set_facecolor('w')\n",
    "ae.draw_neural_net(ax, .1, .9, .1, .9, le, we, be, np, plt)\n",
    "illu_enc = model_file_1 + 'Pics/nn_enc' + model_file_2 + '.pdf'\n",
    "plt.savefig(illu_enc)\n",
    "plt.show()\n",
    "\n",
    "# Decoder\n",
    "fig2 = plt.figure(figsize=(16, 16))\n",
    "ax = fig2.gca()\n",
    "ax.axis('off')\n",
    "ax.set_facecolor('w')\n",
    "ae.draw_neural_net(ax, .1, .9, .1, .9, ld, wd, bd, np, plt)\n",
    "illu_dec = model_file_1 + 'Pics/nn_dec' + model_file_2 + '.pdf'\n",
    "plt.savefig(illu_dec)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
